{"cells":[{"cell_type":"markdown","source":["# Building a Recurrent Neural Network\n","\n","## Sentiment Analysis\n","In this project, we will build a Long Short-term Memory (LSTM) neural network to solve a binary sentiment analysis problem.\n","\n","For this, we'll use the “IMDB Movie Review Dataset\" available on Keras. It includes 50000 highly polarized movie reviews categorized as positive or negative."],"metadata":{"id":"VYmYuGSWFs3-"}},{"cell_type":"markdown","source":["## Importing the required libraries\n","We'll start with importing required libraries.\n","\n","📌 Use the keyword \"import\"."],"metadata":{"id":"jQQ7xy4lzfsw"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"b__mue-XGPZ9","executionInfo":{"status":"ok","timestamp":1677357478780,"user_tz":-180,"elapsed":491,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["# Import TensorFlow\n","import tensorflow as tf\n","\n","# Import NumPy and Matplotlib\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"u0b5YzGHP3qs"},"source":["## Dataset\n","Let's download the IMDB dataset which is included in Keras, and assign it to the corresponding variables *X_train*, *y_train*, *X_test*, and *y_test*. We want to include the most frequently used 10000 words, so we specify 10000 for the num_words parameter.\n","\n","📌 Use the datasets.imdb.load_data() function of the Keras."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"1WLgLQxGGDz8","executionInfo":{"status":"ok","timestamp":1677357486421,"user_tz":-180,"elapsed":4182,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["# Download the IMDB dataset included in Keras\n","# Set the parameter num_words to 10000\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"]},{"cell_type":"markdown","source":["Before we move on, we can print a single sample to see what the data looks like.\n","\n","📌 Use the print() function for this."],"metadata":{"id":"AUPnNCgC0mHm"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"1spB5eY9xh-B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677356045109,"user_tz":-180,"elapsed":583,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}},"outputId":"0508ac5d-0634-49f1-db72-3e9aeadf0dd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"]}],"source":["# Print a sample\n","print(X_train[0])"]},{"cell_type":"markdown","source":["Then, we print the the number of samples in the X_train and X_test datasets to see how the dataset is distributed.\n","\n","📌 Use f-strings for this."],"metadata":{"id":"VKkhznIa8hIw"}},{"cell_type":"code","execution_count":31,"metadata":{"id":"skzb2oTCdV-c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677357491526,"user_tz":-180,"elapsed":475,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}},"outputId":"54febddd-32d7-477c-ae03-b19cf1265443"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train: 25000\n","X_test: 25000\n"]}],"source":["# Print the number of samples\n","print(f\"X_train: {len(X_train)}\")\n","print(f\"X_test: {len(X_test)}\")"]},{"cell_type":"markdown","metadata":{"id":"lF6kV-EsP5vS"},"source":["# Preprocessing\n","### Concatenate\n","\n","To split the dataset with 80-10-10 ratio, we'll first concatenate train and test datasets to create one big dataset.\n","\n","📌 Use contenate() function of the NumPy library for this."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"Whj2C-SlKv2E","executionInfo":{"status":"ok","timestamp":1677357496506,"user_tz":-180,"elapsed":490,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["# Concatenate X_train and X_test and assing it to a variable X\n","X =np.concatenate((X_train, X_test), axis=0)\n","\n","# Concatenate y_train and y_test and assing it to a variable y\n","y =np.concatenate((y_train, y_test), axis=0)"]},{"cell_type":"markdown","metadata":{"id":"ZObXVorUxoGK"},"source":["###Padding\n","\n","Since all reviews are at different lengths, we'll use padding to make all of them same length.\n","\n","📌 Use preprocessing.sequence.pad_sequences() function for this."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"T8mlvy8xKu7-","executionInfo":{"status":"ok","timestamp":1677357500757,"user_tz":-180,"elapsed":1147,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["# Pad all reviews in the X dataset to the length maxlen=1024\n","X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=1024)"]},{"cell_type":"markdown","source":["### Splitting\n","\n","Now, split X and y into train, validation and test dataset and assign those to corresponding values.\n","\n","📌 You can use list slicing methods for this.\n","\n","📌 For this dataset, a 80-10-10 split corresponds to 40000 - 10000 - 10000 number of samples relatively.\n"],"metadata":{"id":"2rZILMK5_-e4"}},{"cell_type":"code","execution_count":34,"metadata":{"id":"Ru_A80XWPr05","executionInfo":{"status":"ok","timestamp":1677357505143,"user_tz":-180,"elapsed":479,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["# Create the training datasets\n","X_train = X[: 40000]\n","y_train = y[: 40000]\n","\n","# Create the validation datasets\n","X_val = X[40000:45000]\n","y_val = y[40000:45000]\n","\n","# Create the test datasets\n","X_test = X[40000:50000]\n","y_test = y[40000:50000]"]},{"cell_type":"markdown","source":["To check if that worked out, print the number of samples in each dataset again.\n","\n","📌 Use f-strings for this."],"metadata":{"id":"E4t0TWEuCs6q"}},{"cell_type":"code","execution_count":35,"metadata":{"id":"yhRLn4stTA4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677357508030,"user_tz":-180,"elapsed":510,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}},"outputId":"ade133e1-a304-4c7a-f4a9-4a0f21f773a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train: 40000\n","y_train: 40000\n","X_val: 5000\n","y_val: 5000\n","X_test: 10000\n","y_test: 10000\n"]}],"source":["# Print the number of samples\n","print(f\"X_train: {len(X_train)}\")\n","print(f\"y_train: {len(y_train)}\")\n","print(f\"X_val: {len(X_val)}\")\n","print(f\"y_val: {len(y_val)}\")\n","print(f\"X_test: {len(X_test)}\")\n","print(f\"y_test: {len(y_test)}\")"]},{"cell_type":"markdown","source":["## Constructing the neural network\n","\n","That was it for the preprocessing of the data! \n","\n","Now we can create our model. First, we start by creating a model object using the Sequential API of Keras.\n","\n","📌 Use tf.keras.Sequential() to create a model object"],"metadata":{"id":"ZDCMa-o8ESLy"}},{"cell_type":"code","execution_count":36,"metadata":{"id":"-lodLU07jdzm","executionInfo":{"status":"ok","timestamp":1677357512049,"user_tz":-180,"elapsed":491,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["model = tf.keras.Sequential()"]},{"cell_type":"markdown","source":["Embedding Layer\n","\n","For the first layer, we add an embedding layer.\n","\n","📌 Use tf.keras.layers.Embedding() for the embedding layer.\n","\n","📌 Use .add() method of the object to add the layer."],"metadata":{"id":"-lh7_MzgFhIf"}},{"cell_type":"code","execution_count":37,"metadata":{"id":"41CLMa1Epasa","executionInfo":{"status":"ok","timestamp":1677357515393,"user_tz":-180,"elapsed":503,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["# Add an embedding layer and a dropout\n","model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=256))\n","model.add(tf.keras.layers.Dropout(0.7))"]},{"cell_type":"markdown","source":[],"metadata":{"id":"pOclZ7n7Qq5j"}},{"cell_type":"markdown","source":["Then, we add a LSTM layer and a dense layer; each with a dropout.\n","\n","📌 Use tf.keras.layers.LSTM() and tf.keras.layers.Dense() to create the layers.\n","\n","📌 Use .add() method of the object to add the layer."],"metadata":{"id":"YpeVhPpEG3u9"}},{"cell_type":"code","execution_count":38,"metadata":{"id":"ntaW1KWrpngU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677357556249,"user_tz":-180,"elapsed":471,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}},"outputId":"9d9a48f6-2937-4884-ac27-01f7670fa842"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}],"source":["# Add a LSTM layer with dropout\n","model.add(tf.keras.layers.LSTM(256, activation=\"relu\"))\n","model.add(tf.keras.layers.Dropout(0.7))\n","\n","# Add a Dense layer with dropout\n","model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n","model.add(tf.keras.layers.Dropout(0.7))"]},{"cell_type":"markdown","source":["### Output layer\n","\n","As the last part of our neural network, we add the output layer. The number of nodes will be one since we are making binary classification. We'll use the sigmoid activation function in the output layer.\n","\n","📌 Use tf.keras.layers.Dense() to create the layer.\n","\n","📌 Use .add() method of the object to add the layer."],"metadata":{"id":"lTWRJxTGHhaI"}},{"cell_type":"code","execution_count":39,"metadata":{"id":"1ufBdJmBs_T-","executionInfo":{"status":"ok","timestamp":1677357621768,"user_tz":-180,"elapsed":485,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["# Add the output layer\n","model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"]},{"cell_type":"markdown","source":["### Optimizer\n","\n","Now we have the structure of our model. To configure the model for training, we'll use the *.compile()* method. Inside the compile method, we have to define the following:\n","*   \"Adam\" for optimizer\n","*   \"Binary Crossentropy\" for the loss function\n","\n","\n","📌 Construct the model with the .compile() method."],"metadata":{"id":"x7EI9LX1I522"}},{"cell_type":"code","execution_count":40,"metadata":{"id":"bkDRiJNW_Dbu","executionInfo":{"status":"ok","timestamp":1677357722642,"user_tz":-180,"elapsed":485,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}}},"outputs":[],"source":["model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"]},{"cell_type":"markdown","source":["## Training the model\n","\n","It's time to train the model. We'll give the X_train and y_train datasets as the first two arguments. These will be used for training. And with the *validation_data* parameter, we'll give the X_val and y_val as a tuple.\n","\n","📌 Use .fit() method of the model object for the training."],"metadata":{"id":"vpcO1HLZJZtZ"}},{"cell_type":"code","execution_count":43,"metadata":{"id":"PoTfLMTt4RQ1","colab":{"base_uri":"https://localhost:8080/","height":470},"executionInfo":{"status":"error","timestamp":1677362439455,"user_tz":-180,"elapsed":2811612,"user":{"displayName":"Fatma Akpunar","userId":"05353608384931707174"}},"outputId":"8654f72c-69cf-40b8-cf18-b836fe5ab314"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1250/1250 [==============================] - 1399s 1s/step - loss: nan - accuracy: 0.5007 - val_loss: nan - val_accuracy: 0.5052\n","Epoch 2/5\n","1250/1250 [==============================] - 1351s 1s/step - loss: nan - accuracy: 0.5007 - val_loss: nan - val_accuracy: 0.5052\n","Epoch 3/5\n","  55/1250 [>.............................] - ETA: 20:48 - loss: nan - accuracy: 0.4903"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-1343dda22461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model for 5 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train the model for 5 epochs\n","results = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"]},{"cell_type":"markdown","source":["### Visualize the results\n","\n","After the model is trained, we can create a graph to visualize the change of loss over time. Results are held in:\n","* results.history[\"loss\"]\n","* results.history[\"val_loss\"]\n","\n","📌 Use plt.show() to display the graph."],"metadata":{"id":"OEx98AYLJwhl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDw7KpHct81z"},"outputs":[],"source":["# Plot the the training loss\n","plt.plot(results.history[\"loss\"], label=\"train\")\n","\n","# Plot the the validation loss\n","plt.plot(results.history[\"val_loss\"], label=\"Validation\")\n","# Name the x and y axises\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","# Put legend table\n","plt.legend()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","source":["Now, do the same thing for accuracy.\n","\n","📌 Accuracy scores can be found in:\n","* results.history[\"accuracy\"]\n","* results.history[\"val_accuracy\"]\n","\n"],"metadata":{"id":"x4f-9V6pKHfE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LUeUQAn_CkD"},"outputs":[],"source":["# Plot the the training accuracy\n","plt.plot(results.history[\"accuracy\"], label=\"train\")\n","\n","# Plot the the validation loaccuracyss\n","plt.plot(results.history[\"val_acruracy\"], label=\"Validation\")\n","# Name the x and y axises\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"accuracy\")\n","# Put legend table\n","plt.legend()\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","source":["## Performance evaluation\n","\n","Let's use the test dataset that we created to evaluate the performance of the model.\n","\n","📌 Use test_on_batch() method with test dataset as parameter."],"metadata":{"id":"xnz14s_zKSq8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"grHvXCZY_JVT"},"outputs":[],"source":["# Evaluate the performance\n","model.evalute(X_test, y_test)"]},{"cell_type":"markdown","source":["### Try a prediction\n","\n","Next, we take a sample and make a prediction on it.\n","\n","📌 Reshape the review to (1, 1024).\n","\n","📌 Use the .prediction() method of the model object."],"metadata":{"id":"MOJH4551KWWe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vda8VhZh_LiK"},"outputs":[],"source":["# Make prediction on the reshaped sample\n","prediction_result = model.predict(X_test[789].reshape(1, 1024))"]},{"cell_type":"code","source":["print(f\"Label: {y_test[789]} | Prediction: {prediction_results}\") #Model çıktımız 0.2. Unutmayın, çıktı 0,5'ten küçükse, 0'a ait deriz. Ve sonuç sıfıra çok yakın olduğu için, modelin bu incelemeyi “negatif” olarak öngördüğünü söyleyebiliriz. Etiket de sıfır olduğundan modelin doğru bir tahmin yaptığı sonucuna varabiliriz."],"metadata":{"id":"cDDjeDGvWi-d"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}